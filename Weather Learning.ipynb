{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire fire_data.txt (35, 256, 512, 1)\n",
      "frac frac_data.txt (35, 256, 512, 1)\n",
      "land ./land_data.txt (35, 256, 512, 1)\n",
      "moisture ./mois_data.txt (35, 256, 512, 1)\n",
      "ndvi ./ndvi_data.txt (35, 256, 512, 1)\n",
      "longwave_energy ./olwr_data.txt (35, 256, 512, 1)\n",
      "(35, 256, 512, 6)\n",
      "(35, 256, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "# The inputs and outptus are the maps downloaded from https://www.nnvl.noaa.gov/view/globaldata.html\n",
    "# The inputs to the model are Land Surface Temperature, Moisture and Longwave Energy. \n",
    "# The output is the average precipitation map. \n",
    "# All maps are in the average weekly format. The idea is to find correlation between the input maps and \n",
    "# the output. \n",
    "\n",
    "input_labels = ['fire', 'frac', 'land', 'moisture', 'ndvi', 'longwave_energy'];\n",
    "files = ['fire_data.txt', 'frac_data.txt', './land_data.txt', './mois_data.txt', './ndvi_data.txt', './olwr_data.txt']\n",
    "input_dict = {}\n",
    "\n",
    "xdim = 256\n",
    "ydim = 512\n",
    "in_chan = 6\n",
    "out_chan = 1\n",
    "\n",
    "# Process the input : input_metrics will contain a 2048 x 4096 image with 9 channels (corresponding to 3 inputs)\n",
    "\n",
    "for label, path in zip(input_labels, files):\n",
    "    \n",
    "    input_dict[label] = [];\n",
    "    input_dict[label] = (np.loadtxt(path,delimiter=',').reshape(-1,xdim,ydim,1))[:35,:,:,:]\n",
    "    print(label, path,input_dict[label].shape)\n",
    "\n",
    "input_metrics = input_dict[input_labels[0]];\n",
    "for label in (input_labels[1:]):\n",
    "    input_metrics = np.concatenate((input_metrics, input_dict[label]), axis=3);\n",
    "\n",
    "# Process the output : output_metric will contain a 256 x 4096 image with 3 channels\n",
    "path = './rain_data.txt'\n",
    "output_metric = (np.loadtxt(path, delimiter=',').reshape(-1,xdim,ydim,1)[:35,:,:,:])\n",
    "output_metric = np.array(output_metric);\n",
    "samples = input_metrics.shape[0];\n",
    "\n",
    "print(input_metrics.shape)\n",
    "print(output_metric.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to obtain the next batch based on the input size and the batch size\n",
    "def next_batch(indices, i):\n",
    "    \n",
    "    ind0, ind1 = i*batch_size, np.minimum((i+1)*batch_size, samples)\n",
    "            \n",
    "    return input_metrics[indices[ind0:ind1], :, :, :], output_metric[indices[ind0:ind1], :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple CNN model that looks like an auto-encoder. This is the section to change for a new model.\n",
    "\n",
    "def conv_net(x):\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, xdim, ydim, in_chan], name='reshape_x');\n",
    "    x = tf.cast(x, tf.float32) \n",
    "    \n",
    "    # Encoder\n",
    "    # 2048x4096x9 -> 1024x2048x32\n",
    "    conv1 = tf.nn.relu(tf.contrib.layers.conv2d(x, 32, [8, 8], stride=2, padding='SAME'))\n",
    "    # 1024x2048x32 -> 512x1024x16\n",
    "    conv2 = tf.nn.relu(tf.contrib.layers.conv2d(conv1, 16, [8, 8], stride=2, padding='SAME'))\n",
    "    # 512x1024x16 -> 256x512x8\n",
    "    conv3 = tf.nn.relu(tf.contrib.layers.conv2d(conv2, 8, [8, 8], stride=2, padding='SAME'))\n",
    "    \n",
    "    # Decoder\n",
    "    # 256x512x8 -> 512x1024x16\n",
    "    conv4 = tf.nn.relu(tf.contrib.layers.conv2d_transpose(conv3, 16, [8, 8], stride=2, padding='SAME'))\n",
    "    # 512x1024x16 -> 1024x2048x32\n",
    "    conv5 = tf.nn.relu(tf.contrib.layers.conv2d_transpose(conv4, 32, [8, 8], stride=2, padding='SAME'))\n",
    "    # 1024x2048x32 -> 2048x4096x3\n",
    "    conv6 = tf.nn.relu(tf.contrib.layers.conv2d_transpose(conv5, out_chan, [8, 8], stride=2, padding='SAME'))\n",
    "    \n",
    "    return conv6;\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape=(None,xdim,ydim,out_chan));\n",
    "X = tf.placeholder(tf.float32, shape=[None,xdim,ydim,in_chan]);\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1);\n",
    "\n",
    "out = conv_net(X);\n",
    "loss_op = tf.reduce_sum(tf.multiply(Y-out,Y-out));\n",
    "train_op = optimizer.minimize(loss_op);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0 [3.0249659e+09]\n",
      "1 [2.0025527e+09]\n",
      "2 [2.5506522e+09]\n",
      "3 [2.2017997e+09]\n",
      "4 [3.0296184e+09]\n",
      "5 [2.8798351e+09]\n",
      "6 [2.723893e+09]\n",
      "7 [2.0725866e+09]\n",
      "8 [2.6847706e+09]\n",
      "9 [2.7234849e+09]\n",
      "10 [3.2241866e+09]\n",
      "11 [2.2098537e+09]\n",
      "12 [3.6641385e+09]\n",
      "13 [2.6662845e+09]\n",
      "14 [2.8257935e+09]\n",
      "15 [2.3834993e+09]\n",
      "16 [2.2946624e+09]\n",
      "17 [2.6696645e+09]\n",
      "18 [2.4607155e+09]\n",
      "19 [2.4007247e+09]\n",
      "20 [3.0113628e+09]\n",
      "21 [2.6558577e+09]\n",
      "22 [2.8963482e+09]\n",
      "23 [2.9861304e+09]\n",
      "24 [2.6723996e+09]\n",
      "25 [2.9446956e+09]\n",
      "26 [2.4405875e+09]\n",
      "27 [2.4430456e+09]\n",
      "28 [2.3430372e+09]\n",
      "29 [2.603906e+09]\n",
      "30 [2.6857178e+09]\n",
      "31 [3.1590057e+09]\n",
      "32 [2.9702428e+09]\n",
      "33 [3.023498e+09]\n",
      "34 [2.3405775e+09]\n",
      "35 [2.8242012e+09]\n",
      "36 [2.4794726e+09]\n",
      "37 [2.596928e+09]\n",
      "38 [2.8512817e+09]\n",
      "39 [3.0350751e+09]\n",
      "40 [2.2044918e+09]\n",
      "41 [2.1067218e+09]\n",
      "42 [1.9339328e+09]\n",
      "43 [2.6844623e+09]\n",
      "44 [2.4665981e+09]\n",
      "45 [2.7954596e+09]\n",
      "46 [2.2914624e+09]\n",
      "47 [2.2221412e+09]\n",
      "48 [2.4618276e+09]\n",
      "49 [2.4642634e+09]\n",
      "50 [2.2503355e+09]\n",
      "51 [2.9391411e+09]\n",
      "52 [2.5056753e+09]\n",
      "53 [2.7251738e+09]\n",
      "54 [2.8356247e+09]\n",
      "55 [2.0242984e+09]\n",
      "56 [2.7094106e+09]\n",
      "57 [2.8196726e+09]\n",
      "58 [2.8153101e+09]\n",
      "59 [3.0682811e+09]\n",
      "60 [2.1530537e+09]\n",
      "61 [3.0482716e+09]\n",
      "62 [2.1757399e+09]\n",
      "63 [2.0073121e+09]\n",
      "64 [2.1835786e+09]\n",
      "65 [2.9974305e+09]\n",
      "66 [3.1712353e+09]\n",
      "67 [2.6570342e+09]\n",
      "68 [2.9195182e+09]\n",
      "69 [2.7718203e+09]\n",
      "70 [2.9137574e+09]\n",
      "71 [3.07777e+09]\n",
      "72 [2.8095711e+09]\n",
      "73 [2.408886e+09]\n",
      "74 [2.7481283e+09]\n",
      "75 [2.7744768e+09]\n",
      "76 [2.2386012e+09]\n",
      "77 [2.7531397e+09]\n",
      "78 [2.4217559e+09]\n",
      "79 [2.8141594e+09]\n",
      "80 [2.4857042e+09]\n",
      "81 [2.7146388e+09]\n",
      "82 [2.645374e+09]\n",
      "83 [1.9634519e+09]\n",
      "84 [2.6692134e+09]\n",
      "85 [2.0810771e+09]\n",
      "86 [3.1340933e+09]\n",
      "87 [2.36591e+09]\n",
      "88 [2.7323884e+09]\n",
      "89 [2.4729375e+09]\n",
      "90 [2.4243397e+09]\n",
      "91 [2.4875558e+09]\n",
      "92 [2.7097623e+09]\n",
      "93 [2.5240891e+09]\n",
      "94 [2.6926272e+09]\n",
      "95 [2.8876375e+09]\n",
      "96 [2.6793467e+09]\n",
      "97 [2.1701486e+09]\n",
      "98 [2.3802967e+09]\n",
      "99 [2.4569211e+09]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100;\n",
    "batch_size = 8;\n",
    "num_batches = int(np.ceil(float(samples) / batch_size));\n",
    "print(num_batches)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        indices = np.random.permutation(samples);\n",
    "    \n",
    "        for i in range(num_batches):\n",
    "            x_train, y_train = next_batch(indices, i);\n",
    "            sess.run([train_op], feed_dict={X: x_train, Y: y_train});            \n",
    "        \n",
    "        loss = sess.run([loss_op], feed_dict={X: x_train, Y: y_train});\n",
    "        print(epoch, loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
